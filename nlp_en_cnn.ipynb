{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp en cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2B2P-6FZYEE",
        "colab_type": "text"
      },
      "source": [
        "드라이브 마운트 확인\n",
        "\n",
        "소스코드 참조\n",
        "\n",
        " https://www.kaggle.com/hamishdickson/cnn-for-sentence-classification-by-yoon-kim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12eMUqc7NkJZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f9007af4-8175-4b2f-a590-8a51b471168b"
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "path = \"/content/drive/My Drive/2020 1학기/자연어처리/friends\"\n",
        "print(os.listdir(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['en_data.csv', 'en_sample.csv', 'friends_train.json', 'friends_dev.json', 'friends_test.json', '.ipynb_checkpoints', 'model.h5', 'model_by_label.h5', 'en_bert_results.csv', 'en_bert_results_round.csv', 'en_results.csv', 'en_results_round.csv', 'en_results_6.csv', 'en_results_round_6.csv', 'en_results_9.csv', 'en_results_round_9.csv', 'en_results_round_3.csv', 'en_results_3.csv', 'lstm_2_model.h5', 'lstm_2_model_25.h5', 'nlp_en_cnn_3.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCfDQioCZVIs",
        "colab_type": "text"
      },
      "source": [
        "임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG3rlFBwOEyY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "867a429b-aacc-4f92-bb4a-5e740f9318af"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t5sPUg25B9s",
        "colab_type": "text"
      },
      "source": [
        "전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hsmlryaOHfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "ea03ed48-b2b5-4798-c5e1-994ac97accff"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# text 를 전처리 해서 쓰려고 만들었는데 막상 적용해보니 성능이 떨어져서 비활성화함\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "# 대략적인 전처리\n",
        "def preprocess_dataframe(df):\n",
        "  df = df.dropna() # none 제거해줌\n",
        "\n",
        "  for i in df.index:\n",
        "    i_utterance = df.at[i, 'i_utterance']\n",
        "    i_dialog = df.at[i, 'i_dialog']\n",
        "    utterance = df.at[i, 'utterance']\n",
        "\n",
        "    if i_utterance == 0: # 새로운 dialog 시작되면\n",
        "      context = \"\" #초기화\n",
        "    else:\n",
        "      context = context + \" \" + utterance\n",
        "    \n",
        "    # 한 dialog 의 텍스트를 다 붙여놓은 형태. 이전 대화의 context도 포함하기 위함\n",
        "    df.at[i, 'context'] = context\n",
        "    \n",
        "  # 길이 \n",
        "  df['l'] = df['context'].apply(lambda x: len(str(x).split(' ')))\n",
        "  print(\"mean length of sentence: \" + str(df.l.mean()))\n",
        "  print(\"max length of sentence: \" + str(df.l.max()))\n",
        "  print(\"std dev length of sentence: \" + str(df.l.std()))\n",
        "  return df\n",
        "\n",
        "# json 파일을 읽어서, kaggle sample csv를 읽은 것처럼 포맷을 정리해준다\n",
        "def read_json_make_dataframe(name='train'):  \n",
        "  file = open('%s/friends_%s.json' % (path, name), encoding='latin1')\n",
        "  d = json.load(file)\n",
        "  data = []\n",
        "  columns = ['i_dialog', 'i_utterance', 'speaker', 'utterance', 'annotation', 'emotion']\n",
        "  i_dialog = -1\n",
        "  for dialog in d:\n",
        "    i_dialog += 1 # 0부터 시작\n",
        "    i_utterance = -1\n",
        "    for row in dialog:\n",
        "      i_utterance += 1 # 0부터 시작\n",
        "      data.append([\n",
        "                   i_dialog, \n",
        "                   i_utterance, \n",
        "                   row['speaker'], \n",
        "                   row['utterance'], \n",
        "                   str(row['annotation']), \n",
        "                   row['emotion']\n",
        "                   ])\n",
        "      \n",
        "        \n",
        "\n",
        "  df = pd.DataFrame(data, columns=columns)\n",
        "  df = preprocess_dataframe(df)\n",
        "  return df\n",
        "\n",
        "# kaggle data set 을 읽기 위한 함수\n",
        "def read_csv_make_dataframe():\n",
        "  df = pd.read_csv('%s/en_data.csv' % path, encoding='latin1')\n",
        "  df = preprocess_dataframe(df)\n",
        "  df = df[['i_dialog', 'i_utterance', 'speaker', 'utterance', 'context']]\n",
        "  return df\n",
        "\n",
        "\n",
        "train = read_json_make_dataframe('train')\n",
        "print(train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "mean length of sentence: 63.43404980588959\n",
            "max length of sentence: 303\n",
            "std dev length of sentence: 49.02061864789282\n",
            "       i_dialog  ...    l\n",
            "0             0  ...    1\n",
            "1             0  ...    7\n",
            "2             0  ...   13\n",
            "3             0  ...   22\n",
            "4             0  ...   27\n",
            "...         ...  ...  ...\n",
            "10556       719  ...  129\n",
            "10557       719  ...  139\n",
            "10558       719  ...  146\n",
            "10559       719  ...  147\n",
            "10560       719  ...  160\n",
            "\n",
            "[10561 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM-wiwuxtSdO",
        "colab_type": "text"
      },
      "source": [
        "annotaion 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C5l8FHPtTI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# annotation 값을 emotion 값으로 변환함\n",
        "def annotation_to_emotion(annotation):\n",
        "    # 반올림 기반 변환법\n",
        "\n",
        "    # 5 곱해서 반올림 친 벡터를 만듦\n",
        "    rounded = [round(a * 5) for a in annotation]    \n",
        "\n",
        "    # non-neutral check\n",
        "    # 최대치가 같은게 두개 이상 있으면 neutral\n",
        "    max_value = max(rounded)\n",
        "    if len([x for x in rounded if x == max_value]) >= 2:\n",
        "      return \"non-neutral\"\n",
        "    \n",
        "    # 맥스 밸류 인덱스에 해당하는 이모션 리턴\n",
        "    return ['neutral','joy', 'sadness', 'fear', 'anger', 'surprise', 'disgust'][rounded.index(max_value)]\n",
        "\n",
        "def summary_y_hats(y_hats):\n",
        "  # y_hats의 분포를 보여주는 함수\n",
        "  count_dict = {}\n",
        "  \n",
        "  for i in range(0, len(y_hats)):\n",
        "    y_hat = y_hats[i]\n",
        "    prediected_emotion = annotation_to_emotion(y_hat) # 예측한 이모션 값\n",
        "    count_dict[prediected_emotion] = count_dict.get(prediected_emotion, 0) + 1\n",
        "\n",
        "  print(\"summary of y:\", count_dict)\n",
        "  return\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSO2_ioftwJ-",
        "colab_type": "text"
      },
      "source": [
        "러닝 프로세스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67F7Wxuqtxay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측\n",
        "def predict(data):\n",
        "  from sklearn.metrics import f1_score, accuracy_score\n",
        "  count = len(data)\n",
        "  X = generate_X(data)\n",
        "  y = generate_Y(data) # 정답\n",
        "  y_hats = model.predict(X) # 예측치(annotation 벡터)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"summary of y\")\n",
        "  summary_y_hats(y)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"summary of y_hats\")\n",
        "  summary_y_hats(y_hats)\n",
        "\n",
        "  y_true = [x for x in data['emotion'].values]\n",
        "  y_pred = [annotation_to_emotion(x) for x in y_hats]\n",
        "\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  print(\"accuray: \", acc)\n",
        "  print(\"f1_score: \",f1_score(y_true, y_pred, average='macro'))\n",
        "  return acc\n",
        "\n",
        "def run(index=\"\"):\n",
        "  data = read_csv_make_dataframe()\n",
        "\n",
        "  count = len(data)\n",
        "  X = generate_X(data)\n",
        "  y_hats = model.predict(X) # 예측치(annotation 벡터)\n",
        "\n",
        "  results = []\n",
        "  columns = ['Id', 'Predicted']\n",
        "  for i in range(0, count):\n",
        "    y_hat = y_hats[i]\n",
        "    prediected_emotion = annotation_to_emotion(y_hat) # 예측한 이모션 값\n",
        "\n",
        "    results.append((i, prediected_emotion))\n",
        "\n",
        "  pd.DataFrame(results, columns=columns).to_csv('%s/nlp_en_cnn_%s.csv' % (path, index), index=False)\n",
        "  return\n",
        "\n",
        "def learn():\n",
        "  # 테스트 데이터 읽음\n",
        "  test = read_json_make_dataframe('test')\n",
        "  acc = predict(test)\n",
        "  accuracies = []\n",
        "  for i in range(0, 3):\n",
        "    epochs = 3\n",
        "    batch_size = 32\n",
        "\n",
        "    history = model.fit(X_train, \n",
        "                        y_train, \n",
        "                        epochs=epochs, \n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "    )\n",
        "    acc = predict(test)\n",
        "    accuracies.append(acc)\n",
        "    learned = epochs * (i+1)\n",
        "    run(learned)\n",
        "\n",
        "  print(\"seq of accuracies:\", accuracies)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp9mfXw4aY7C",
        "colab_type": "text"
      },
      "source": [
        "읽은 인풋으로 러닝 데이터를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4f6Vcubegi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these sentences aren't that long so we may as well use the whole string\n",
        "sequence_length = 70 # 이 길이를 넘은 문장은 걍 짤림\n",
        "max_features = 20000 #len(embeddings_index) # this is the number of words we care about\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', lower=False)\n",
        "\n",
        "# X 데이터를 만들어줌\n",
        "def generate_X(df):\n",
        "  # tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', lower=False)\n",
        "  tokenizer.fit_on_texts(df['utterance'].values)\n",
        "\n",
        "  # this takes our sentences and replaces each word with an integer\n",
        "  X = tokenizer.texts_to_sequences(df['utterance'].values)\n",
        "\n",
        "  # we then pad the sequences so they're all the same length (sequence_length)\n",
        "  X = pad_sequences(X, sequence_length)\n",
        "  return X\n",
        "\n",
        "# Y 데이터를 만들어줌\n",
        "def generate_Y(df):\n",
        "  annotations = df['annotation'].apply(lambda x: pd.Series([float(c)/5 for c in x], dtype='float32'))\n",
        "  # 어노테이션 값을 벡터화함.\n",
        "  # 2120000 ->벡터화-> [2, 1, 2, 0, 0, 0, 0] ->나누기 5-> [0.4, 0.2, 0.4, 0, 0, 0, 0]\n",
        "  return annotations.values\n",
        "\n",
        "X_train = generate_X(train)\n",
        "y_train = generate_Y(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmXGxA_hbF4_",
        "colab_type": "text"
      },
      "source": [
        "모델이 있다면 로딩하고, 없으면 생성해봄"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwEWXpRJlwr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "b5824d60-c558-49d9-bcc1-d51a0eb9ff5e"
      },
      "source": [
        "embedding_dim = 300 # Kim uses 300 here\n",
        "num_filters = 100\n",
        "\n",
        "def create_model():\n",
        "  inputs = Input(shape=(sequence_length,), dtype='int32')\n",
        "\n",
        "  # use a random embedding for the text\n",
        "  embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
        "  reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n",
        "  from tensorflow.python.keras import regularizers\n",
        "  # Note the relu activation which Kim specifically mentions\n",
        "  # He also uses an l2 constraint of 3\n",
        "  # Also, note that the convolution window acts on the whole 300 dimensions - that's important\n",
        "  conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
        "  conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
        "  conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
        "\n",
        "  # perform max pooling on each of the convoluations\n",
        "  maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "  maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "  maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "  \n",
        "  # concat and flatten\n",
        "  concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2,])\n",
        "  flatten = Flatten()(concatenated_tensor)\n",
        "\n",
        "  # do dropout and predict\n",
        "  dropout = Dropout(0.5)(flatten)\n",
        "  output = Dense(units=7, activation='softmax')(dropout)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=output)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 70)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 70, 300)      6000000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 70, 300, 1)   0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 68, 1, 100)   90100       reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 67, 1, 100)   120100      reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 66, 1, 100)   150100      reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3, 1, 100)    0           max_pooling2d_1[0][0]            \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 300)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 300)          0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            2107        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,362,407\n",
            "Trainable params: 6,362,407\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc0O0IgqbTEH",
        "colab_type": "text"
      },
      "source": [
        "실행\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CWzWBcQ562_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09bd32b3-55bb-443a-87bc-86a448581232"
      },
      "source": [
        "learn()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean length of sentence: 60.91280752532562\n",
            "max length of sentence: 286\n",
            "std dev length of sentence: 47.66618446797404\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'non-neutral': 2764}\n",
            "accuray:  0.19573082489146165\n",
            "f1_score:  0.040922844175491684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "10561/10561 [==============================] - 4s 355us/step - loss: 1.8528 - accuracy: 0.5504\n",
            "Epoch 2/3\n",
            "10561/10561 [==============================] - 3s 328us/step - loss: 1.6228 - accuracy: 0.5514\n",
            "Epoch 3/3\n",
            "10561/10561 [==============================] - 3s 328us/step - loss: 1.6323 - accuracy: 0.5514\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'neutral': 2764}\n",
            "accuray:  0.46562952243125905\n",
            "f1_score:  0.07942483337447544\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "Epoch 1/3\n",
            "10561/10561 [==============================] - 3s 330us/step - loss: 1.6383 - accuracy: 0.5514\n",
            "Epoch 2/3\n",
            "10561/10561 [==============================] - 4s 333us/step - loss: 1.6421 - accuracy: 0.5514\n",
            "Epoch 3/3\n",
            "10561/10561 [==============================] - 3s 331us/step - loss: 1.6415 - accuracy: 0.5514\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'neutral': 2764}\n",
            "accuray:  0.46562952243125905\n",
            "f1_score:  0.07942483337447544\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "Epoch 1/3\n",
            "10561/10561 [==============================] - 4s 336us/step - loss: 1.6389 - accuracy: 0.5514\n",
            "Epoch 2/3\n",
            "10561/10561 [==============================] - 4s 340us/step - loss: 1.6348 - accuracy: 0.5514\n",
            "Epoch 3/3\n",
            "10561/10561 [==============================] - 4s 341us/step - loss: 1.6389 - accuracy: 0.5509\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'neutral': 2764}\n",
            "accuray:  0.46562952243125905\n",
            "f1_score:  0.07942483337447544\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "seq of accuracies: [0.46562952243125905, 0.46562952243125905, 0.46562952243125905]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
