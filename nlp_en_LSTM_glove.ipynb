{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp en LSTM glove",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2B2P-6FZYEE",
        "colab_type": "text"
      },
      "source": [
        "드라이브 마운트 확인\n",
        "\n",
        "소스코드 참조\n",
        "\n",
        "https://blog.naver.com/PostView.nhn?blogId=hist0134&logNo=220944328300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12eMUqc7NkJZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "bc0af91d-108a-4495-c144-624288bebdf0"
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "path = \"/content/drive/My Drive/2020 1학기/자연어처리/friends\"\n",
        "print(os.listdir(path))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['en_data.csv', 'en_sample.csv', 'friends_train.json', 'friends_dev.json', 'friends_test.json', '.ipynb_checkpoints', 'model.h5', 'model_by_label.h5', 'en_bert_results.csv', 'en_bert_results_round.csv', 'en_results.csv', 'en_results_round.csv', 'en_results_6.csv', 'en_results_round_6.csv', 'en_results_9.csv', 'en_results_round_9.csv', 'en_results_round_3.csv', 'en_results_3.csv', 'lstm_2_model.h5', 'lstm_2_model_25.h5', 'nlp_en_cnn_3.csv', 'nlp_en_cnn_6.csv', 'nlp_en_cnn_9.csv', 'nlp_en_Dual_Encoder_LSTM_1.csv', 'nlp_en_Dual_Encoder_LSTM_3.csv', 'nlp_en_bert_3.csv', 'nlp_en_Dual_Encoder_LSTM_6.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCfDQioCZVIs",
        "colab_type": "text"
      },
      "source": [
        "임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG3rlFBwOEyY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "506bfd9f-f237-45d4-a0f6-6d9ab2198403"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t5sPUg25B9s",
        "colab_type": "text"
      },
      "source": [
        "전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hsmlryaOHfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "67b906ae-ac81-41c9-ccbb-87f59c9d2573"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# text 를 전처리 해서 쓰려고 만들었는데 막상 적용해보니 성능이 떨어져서 비활성화함\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "# 대략적인 전처리\n",
        "def preprocess_dataframe(df):\n",
        "  df = df.dropna() # none 제거해줌\n",
        "\n",
        "  for i in df.index:\n",
        "    i_utterance = df.at[i, 'i_utterance']\n",
        "    i_dialog = df.at[i, 'i_dialog']\n",
        "    utterance = df.at[i, 'utterance']\n",
        "\n",
        "    if i_utterance == 0: # 새로운 dialog 시작되면\n",
        "      context = \"\" #초기화\n",
        "    else:\n",
        "      context = context + \" \" + utterance\n",
        "    \n",
        "    # 한 dialog 의 텍스트를 다 붙여놓은 형태. 이전 대화의 context도 포함하기 위함\n",
        "    df.at[i, 'context'] = context\n",
        "    \n",
        "  # 길이 \n",
        "  df['l'] = df['context'].apply(lambda x: len(str(x).split(' ')))\n",
        "  print(\"mean length of sentence: \" + str(df.l.mean()))\n",
        "  print(\"max length of sentence: \" + str(df.l.max()))\n",
        "  print(\"std dev length of sentence: \" + str(df.l.std()))\n",
        "  return df\n",
        "\n",
        "# json 파일을 읽어서, kaggle sample csv를 읽은 것처럼 포맷을 정리해준다\n",
        "def read_json_make_dataframe(name='train'):  \n",
        "  file = open('%s/friends_%s.json' % (path, name), encoding='latin1')\n",
        "  d = json.load(file)\n",
        "  data = []\n",
        "  columns = ['i_dialog', 'i_utterance', 'speaker', 'utterance', 'annotation', 'emotion']\n",
        "  i_dialog = -1\n",
        "  for dialog in d:\n",
        "    i_dialog += 1 # 0부터 시작\n",
        "    i_utterance = -1\n",
        "    for row in dialog:\n",
        "      i_utterance += 1 # 0부터 시작\n",
        "      data.append([\n",
        "                   i_dialog, \n",
        "                   i_utterance, \n",
        "                   row['speaker'], \n",
        "                   row['utterance'], \n",
        "                   str(row['annotation']), \n",
        "                   row['emotion']\n",
        "                   ])\n",
        "      \n",
        "        \n",
        "\n",
        "  df = pd.DataFrame(data, columns=columns)\n",
        "  df = preprocess_dataframe(df)\n",
        "  return df\n",
        "\n",
        "# kaggle data set 을 읽기 위한 함수\n",
        "def read_csv_make_dataframe():\n",
        "  df = pd.read_csv('%s/en_data.csv' % path, encoding='latin1')\n",
        "  df = preprocess_dataframe(df)\n",
        "  df = df[['i_dialog', 'i_utterance', 'speaker', 'utterance', 'context']]\n",
        "  return df\n",
        "\n",
        "\n",
        "train = read_json_make_dataframe('train')\n",
        "print(train)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "mean length of sentence: 63.43404980588959\n",
            "max length of sentence: 303\n",
            "std dev length of sentence: 49.02061864789282\n",
            "       i_dialog  ...    l\n",
            "0             0  ...    1\n",
            "1             0  ...    7\n",
            "2             0  ...   13\n",
            "3             0  ...   22\n",
            "4             0  ...   27\n",
            "...         ...  ...  ...\n",
            "10556       719  ...  129\n",
            "10557       719  ...  139\n",
            "10558       719  ...  146\n",
            "10559       719  ...  147\n",
            "10560       719  ...  160\n",
            "\n",
            "[10561 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM-wiwuxtSdO",
        "colab_type": "text"
      },
      "source": [
        "annotaion 변환 및 러닝 프로세스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C5l8FHPtTI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# annotation 값을 emotion 값으로 변환함\n",
        "def annotation_to_emotion(annotation):\n",
        "    # 반올림 기반 변환법\n",
        "\n",
        "    # 5 곱해서 반올림 친 벡터를 만듦\n",
        "    rounded = [round(a * 5) for a in annotation]    \n",
        "\n",
        "    # non-neutral check\n",
        "    # 최대치가 같은게 두개 이상 있으면 neutral\n",
        "    max_value = max(rounded)\n",
        "    if len([x for x in rounded if x == max_value]) >= 2:\n",
        "      return \"non-neutral\"\n",
        "    \n",
        "    # 맥스 밸류 인덱스에 해당하는 이모션 리턴\n",
        "    return ['neutral','joy', 'sadness', 'fear', 'anger', 'surprise', 'disgust'][rounded.index(max_value)]\n",
        "\n",
        "def summary_y_hats(y_hats):\n",
        "  # y_hats의 분포를 보여주는 함수\n",
        "  count_dict = {}\n",
        "  \n",
        "  for i in range(0, len(y_hats)):\n",
        "    y_hat = y_hats[i]\n",
        "    prediected_emotion = annotation_to_emotion(y_hat) # 예측한 이모션 값\n",
        "    count_dict[prediected_emotion] = count_dict.get(prediected_emotion, 0) + 1\n",
        "\n",
        "  print(\"summary of y:\", count_dict)\n",
        "  return\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSO2_ioftwJ-",
        "colab_type": "text"
      },
      "source": [
        "러닝 프로세스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67F7Wxuqtxay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측\n",
        "def predict(data):\n",
        "  from sklearn.metrics import f1_score, accuracy_score\n",
        "  count = len(data)\n",
        "  X = generate_X(data)\n",
        "  y = generate_Y(data) # 정답\n",
        "  y_hats = model.predict(X) # 예측치(annotation 벡터)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"summary of y\")\n",
        "  summary_y_hats(y)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"summary of y_hats\")\n",
        "  summary_y_hats(y_hats)\n",
        "\n",
        "  y_true = [x for x in data['emotion'].values]\n",
        "  y_pred = [annotation_to_emotion(x) for x in y_hats]\n",
        "\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  print(\"accuray: \", acc)\n",
        "  print(\"f1_score: \",f1_score(y_true, y_pred, average='macro'))\n",
        "  return acc\n",
        "\n",
        "def run(index=\"\"):\n",
        "  data = read_csv_make_dataframe()\n",
        "\n",
        "  count = len(data)\n",
        "  X = generate_X(data)\n",
        "  y_hats = model.predict(X) # 예측치(annotation 벡터)\n",
        "\n",
        "  results = []\n",
        "  columns = ['Id', 'Predicted']\n",
        "  for i in range(0, count):\n",
        "    y_hat = y_hats[i]\n",
        "    prediected_emotion = annotation_to_emotion(y_hat) # 예측한 이모션 값\n",
        "\n",
        "    results.append((i, prediected_emotion))\n",
        "\n",
        "  pd.DataFrame(results, columns=columns).to_csv('%s/nlp_en_LSTM_glove_%s.csv' % (path, index), index=False)\n",
        "  return\n",
        "\n",
        "def learn():\n",
        "  # 테스트 데이터 읽음\n",
        "  test = read_json_make_dataframe('test')\n",
        "  acc = predict(test)\n",
        "  accuracies = []\n",
        "  for i in range(0, 3):\n",
        "    epochs = 3\n",
        "    batch_size = 256\n",
        "\n",
        "    history = model.fit(X_train, \n",
        "                        y_train, \n",
        "                        epochs=epochs, \n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "    )\n",
        "    acc = predict(test)\n",
        "    accuracies.append(acc)\n",
        "    learned = epochs * (i+1)\n",
        "    run(learned)\n",
        "\n",
        "  print(\"seq of accuracies:\", accuracies)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp9mfXw4aY7C",
        "colab_type": "text"
      },
      "source": [
        "읽은 인풋으로 러닝 데이터를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4f6Vcubegi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c082d0c9-6bcb-409f-b7ff-f7d49c9107ae"
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 350\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train['context'].values)\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = max([len(seq) for seq in train['context'].values])\n",
        "MAX_NB_WORDS = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "def generate_X(df):\n",
        "  X = tokenizer.texts_to_sequences(df['context'].values)\n",
        "  X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  print('Shape of data tensor:', X.shape)\n",
        "  return X\n",
        "\n",
        "\n",
        "# Y 데이터를 만들어줌\n",
        "def generate_Y(df):\n",
        "  annotations = df['annotation'].apply(lambda x: pd.Series([float(c)/5 for c in x], dtype='float32'))\n",
        "  # 어노테이션 값을 벡터화함.\n",
        "  # 2120000 ->벡터화-> [2, 1, 2, 0, 0, 0, 0] ->나누기 5-> [0.4, 0.2, 0.4, 0, 0, 0, 0]\n",
        "  return annotations.values\n",
        "\n",
        "X_train = generate_X(train)\n",
        "y_train = generate_Y(train)\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5807 unique tokens.\n",
            "Shape of data tensor: (10561, 1501)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k-SuS7N3vje",
        "colab_type": "text"
      },
      "source": [
        "glove 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ozf1hUP3wn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "36191915-6bb9-454f-fb64-da551f204332"
      },
      "source": [
        "# embedding matrix 에 사용하기 위해 pre trained vector 사용.\n",
        "# golve 300d 데이터 사용\n",
        "try:\n",
        "  f = open('glove.6B.300d.txt', encoding='utf-8')\n",
        "except:\n",
        "  !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "  !unzip glove*.zip\n",
        "\n",
        "  !ls\n",
        "  !pwd\n",
        "  \n",
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(\"glove.6B.{}d.txt\".format(EMBEDDING_DIM), encoding='utf8') as fp:\n",
        "    for line in fp:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((MAX_NB_WORDS, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmXGxA_hbF4_",
        "colab_type": "text"
      },
      "source": [
        "모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwEWXpRJlwr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  embedding = Embedding(output_dim=EMBEDDING_DIM,\n",
        "                      input_dim=MAX_NB_WORDS,\n",
        "                      input_length=MAX_SEQUENCE_LENGTH,\n",
        "                      weights=[embedding_matrix],\n",
        "                      mask_zero=True,\n",
        "                      trainable=False)\n",
        "\n",
        "  model.add(embedding)\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(7, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc0O0IgqbTEH",
        "colab_type": "text"
      },
      "source": [
        "실행\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CWzWBcQ562_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24ac2f43-d356-4f67-9c63-a150bedca0ac"
      },
      "source": [
        "learn()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean length of sentence: 60.91280752532562\n",
            "max length of sentence: 286\n",
            "std dev length of sentence: 47.66618446797404\n",
            "Shape of data tensor: (2764, 1501)\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'non-neutral': 2763, 'sadness': 1}\n",
            "accuray:  0.19573082489146165\n",
            "f1_score:  0.04093523002421307\n",
            "Epoch 1/3\n",
            "10561/10561 [==============================] - 134s 13ms/step - loss: 1.6695 - accuracy: 0.5127\n",
            "Epoch 2/3\n",
            "10561/10561 [==============================] - 133s 13ms/step - loss: 1.5875 - accuracy: 0.5594\n",
            "Epoch 3/3\n",
            "10561/10561 [==============================] - 133s 13ms/step - loss: 1.5592 - accuracy: 0.5660\n",
            "Shape of data tensor: (2764, 1501)\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'non-neutral': 473, 'neutral': 2189, 'joy': 50, 'surprise': 49, 'sadness': 3}\n",
            "accuray:  0.42908827785817655\n",
            "f1_score:  0.13528432957310912\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "Shape of data tensor: (3296, 1501)\n",
            "Epoch 1/3\n",
            "10561/10561 [==============================] - 133s 13ms/step - loss: 1.5366 - accuracy: 0.5689\n",
            "Epoch 2/3\n",
            "10561/10561 [==============================] - 133s 13ms/step - loss: 1.5239 - accuracy: 0.5725\n",
            "Epoch 3/3\n",
            "10561/10561 [==============================] - 133s 13ms/step - loss: 1.5097 - accuracy: 0.5744\n",
            "Shape of data tensor: (2764, 1501)\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'non-neutral': 494, 'neutral': 2126, 'joy': 51, 'surprise': 76, 'sadness': 12, 'anger': 3, 'disgust': 2}\n",
            "accuray:  0.44247467438494936\n",
            "f1_score:  0.1619877912969879\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "Shape of data tensor: (3296, 1501)\n",
            "Epoch 1/3\n",
            "10561/10561 [==============================] - 133s 13ms/step - loss: 1.4999 - accuracy: 0.5761\n",
            "Epoch 2/3\n",
            "10561/10561 [==============================] - 133s 13ms/step - loss: 1.4891 - accuracy: 0.5812\n",
            "Epoch 3/3\n",
            "10561/10561 [==============================] - 134s 13ms/step - loss: 1.4807 - accuracy: 0.5829\n",
            "Shape of data tensor: (2764, 1501)\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'non-neutral': 515, 'neutral': 2074, 'joy': 68, 'surprise': 83, 'sadness': 14, 'anger': 4, 'disgust': 6}\n",
            "accuray:  0.4435600578871201\n",
            "f1_score:  0.17665112918535755\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "Shape of data tensor: (3296, 1501)\n",
            "seq of accuracies: [0.42908827785817655, 0.44247467438494936, 0.4435600578871201]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
