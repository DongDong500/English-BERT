{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp en Bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2B2P-6FZYEE",
        "colab_type": "text"
      },
      "source": [
        "드라이브 마운트 확인\n",
        "\n",
        "소스코드 참조\n",
        "\n",
        " https://www.kaggle.com/sharmilaupadhyaya/20newsgroup-classification-using-keras-bert-in-gpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12eMUqc7NkJZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f18e6a39-a82d-4581-f120-b4870f795717"
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "path = \"/content/drive/My Drive/2020 1학기/자연어처리/friends\"\n",
        "print(os.listdir(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['en_data.csv', 'en_sample.csv', 'friends_train.json', 'friends_dev.json', 'friends_test.json', '.ipynb_checkpoints', 'model.h5', 'model_by_label.h5', 'en_bert_results.csv', 'en_bert_results_round.csv', 'en_results.csv', 'en_results_round.csv', 'en_results_6.csv', 'en_results_round_6.csv', 'en_results_9.csv', 'en_results_round_9.csv', 'en_results_round_3.csv', 'en_results_3.csv', 'lstm_2_model.h5', 'lstm_2_model_25.h5', 'nlp_en_cnn_3.csv', 'nlp_en_cnn_6.csv', 'nlp_en_cnn_9.csv', 'nlp_en_Dual_Encoder_LSTM_1.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCfDQioCZVIs",
        "colab_type": "text"
      },
      "source": [
        "임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG3rlFBwOEyY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f1d9ce6-f6ed-4267-edaf-50099711b0ef"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t5sPUg25B9s",
        "colab_type": "text"
      },
      "source": [
        "전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hsmlryaOHfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "adddad09-da87-4bd8-dbcc-255ff5985034"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# text 를 전처리 해서 쓰려고 만들었는데 막상 적용해보니 성능이 떨어져서 비활성화함\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "# 대략적인 전처리\n",
        "def preprocess_dataframe(df):\n",
        "  df = df.dropna() # none 제거해줌\n",
        "\n",
        "  for i in df.index:\n",
        "    i_utterance = df.at[i, 'i_utterance']\n",
        "    i_dialog = df.at[i, 'i_dialog']\n",
        "    utterance = df.at[i, 'utterance']\n",
        "\n",
        "    if i_utterance == 0: # 새로운 dialog 시작되면\n",
        "      context = \"\" #초기화\n",
        "    else:\n",
        "      context = context + \" \" + utterance\n",
        "    \n",
        "    # 한 dialog 의 텍스트를 다 붙여놓은 형태. 이전 대화의 context도 포함하기 위함\n",
        "    df.at[i, 'context'] = context\n",
        "    \n",
        "  # 길이 \n",
        "  df['l'] = df['context'].apply(lambda x: len(str(x).split(' ')))\n",
        "  print(\"mean length of sentence: \" + str(df.l.mean()))\n",
        "  print(\"max length of sentence: \" + str(df.l.max()))\n",
        "  print(\"std dev length of sentence: \" + str(df.l.std()))\n",
        "  return df\n",
        "\n",
        "# json 파일을 읽어서, kaggle sample csv를 읽은 것처럼 포맷을 정리해준다\n",
        "def read_json_make_dataframe(name='train'):  \n",
        "  file = open('%s/friends_%s.json' % (path, name), encoding='latin1')\n",
        "  d = json.load(file)\n",
        "  data = []\n",
        "  columns = ['i_dialog', 'i_utterance', 'speaker', 'utterance', 'annotation', 'emotion']\n",
        "  i_dialog = -1\n",
        "  for dialog in d:\n",
        "    i_dialog += 1 # 0부터 시작\n",
        "    i_utterance = -1\n",
        "    for row in dialog:\n",
        "      i_utterance += 1 # 0부터 시작\n",
        "      data.append([\n",
        "                   i_dialog, \n",
        "                   i_utterance, \n",
        "                   row['speaker'], \n",
        "                   row['utterance'], \n",
        "                   str(row['annotation']), \n",
        "                   row['emotion']\n",
        "                   ])\n",
        "      \n",
        "        \n",
        "\n",
        "  df = pd.DataFrame(data, columns=columns)\n",
        "  df = preprocess_dataframe(df)\n",
        "  return df\n",
        "\n",
        "# kaggle data set 을 읽기 위한 함수\n",
        "def read_csv_make_dataframe():\n",
        "  df = pd.read_csv('%s/en_data.csv' % path, encoding='latin1')\n",
        "  df = preprocess_dataframe(df)\n",
        "  df = df[['i_dialog', 'i_utterance', 'speaker', 'utterance', 'context']]\n",
        "  return df\n",
        "\n",
        "\n",
        "train = read_json_make_dataframe('train')\n",
        "print(train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "mean length of sentence: 63.43404980588959\n",
            "max length of sentence: 303\n",
            "std dev length of sentence: 49.02061864789282\n",
            "       i_dialog  ...    l\n",
            "0             0  ...    1\n",
            "1             0  ...    7\n",
            "2             0  ...   13\n",
            "3             0  ...   22\n",
            "4             0  ...   27\n",
            "...         ...  ...  ...\n",
            "10556       719  ...  129\n",
            "10557       719  ...  139\n",
            "10558       719  ...  146\n",
            "10559       719  ...  147\n",
            "10560       719  ...  160\n",
            "\n",
            "[10561 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM-wiwuxtSdO",
        "colab_type": "text"
      },
      "source": [
        "annotaion 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C5l8FHPtTI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# annotation 값을 emotion 값으로 변환함\n",
        "def annotation_to_emotion(annotation):\n",
        "    # 반올림 기반 변환법\n",
        "\n",
        "    # 5 곱해서 반올림 친 벡터를 만듦\n",
        "    rounded = [round(a * 5) for a in annotation]    \n",
        "\n",
        "    # non-neutral check\n",
        "    # 최대치가 같은게 두개 이상 있으면 neutral\n",
        "    max_value = max(rounded)\n",
        "    if len([x for x in rounded if x == max_value]) >= 2:\n",
        "      return \"non-neutral\"\n",
        "    \n",
        "    # 맥스 밸류 인덱스에 해당하는 이모션 리턴\n",
        "    return ['neutral','joy', 'sadness', 'fear', 'anger', 'surprise', 'disgust'][rounded.index(max_value)]\n",
        "\n",
        "def summary_y_hats(y_hats):\n",
        "  # y_hats의 분포를 보여주는 함수\n",
        "  count_dict = {}\n",
        "  \n",
        "  for i in range(0, len(y_hats)):\n",
        "    y_hat = y_hats[i]\n",
        "    prediected_emotion = annotation_to_emotion(y_hat) # 예측한 이모션 값\n",
        "    count_dict[prediected_emotion] = count_dict.get(prediected_emotion, 0) + 1\n",
        "\n",
        "  print(\"summary of y:\", count_dict)\n",
        "  return\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSO2_ioftwJ-",
        "colab_type": "text"
      },
      "source": [
        "러닝 프로세스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67F7Wxuqtxay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측\n",
        "def predict(data):\n",
        "  from sklearn.metrics import f1_score, accuracy_score\n",
        "  count = len(data)\n",
        "  X = generate_X(data)\n",
        "  y = generate_Y(data) # 정답\n",
        "  y_hats = model.predict(X) # 예측치(annotation 벡터)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"summary of y\")\n",
        "  summary_y_hats(y)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"summary of y_hats\")\n",
        "  summary_y_hats(y_hats)\n",
        "\n",
        "  y_true = [x for x in data['emotion'].values]\n",
        "  y_pred = [annotation_to_emotion(x) for x in y_hats]\n",
        "\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  print(\"accuray: \", acc)\n",
        "  print(\"f1_score: \",f1_score(y_true, y_pred, average='macro'))\n",
        "  return acc\n",
        "\n",
        "def run(index=\"\"):\n",
        "  data = read_csv_make_dataframe()\n",
        "\n",
        "  count = len(data)\n",
        "  X = generate_X(data)\n",
        "  y_hats = model.predict(X) # 예측치(annotation 벡터)\n",
        "\n",
        "  results = []\n",
        "  columns = ['Id', 'Predicted']\n",
        "  for i in range(0, count):\n",
        "    y_hat = y_hats[i]\n",
        "    prediected_emotion = annotation_to_emotion(y_hat) # 예측한 이모션 값\n",
        "\n",
        "    results.append((i, prediected_emotion))\n",
        "\n",
        "  pd.DataFrame(results, columns=columns).to_csv('%s/nlp_en_bert_%s.csv' % (path, index), index=False)\n",
        "  return\n",
        "\n",
        "def learn():\n",
        "  # 테스트 데이터 읽음\n",
        "  test = read_json_make_dataframe('test')\n",
        "  acc = predict(test)\n",
        "  accuracies = []\n",
        "  for i in range(0, 1):\n",
        "    epochs = 3\n",
        "    batch_size = 32\n",
        "\n",
        "    history = model.fit(X_train, \n",
        "                        y_train, \n",
        "                        epochs=epochs, \n",
        "                        batch_size=batch_size\n",
        "    )\n",
        "    acc = predict(test)\n",
        "    accuracies.append(acc)\n",
        "    learned = epochs * (i+1)\n",
        "    run(learned)\n",
        "\n",
        "  print(\"seq of accuracies:\", accuracies)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c_KGHYEzGUq",
        "colab_type": "text"
      },
      "source": [
        "라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6qysYqCzEqo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "a2979d29-501b-4827-f8c5-0039c1c3a77d"
      },
      "source": [
        "!pip install keras-bert\n",
        "!pip install keras-rectified-adam\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-bert in /usr/local/lib/python3.6/dist-packages (0.84.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.5)\n",
            "Requirement already satisfied: keras-transformer>=0.37.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (0.37.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert) (0.14.0)\n",
            "Requirement already satisfied: keras-pos-embd>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert) (0.11.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert) (0.6.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.27.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert) (0.27.0)\n",
            "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert) (0.7.0)\n",
            "Requirement already satisfied: keras-self-attention==0.46.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.27.0->keras-transformer>=0.37.0->keras-bert) (0.46.0)\n",
            "Requirement already satisfied: keras-rectified-adam in /usr/local/lib/python3.6/dist-packages (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-rectified-adam) (1.18.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-rectified-adam) (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp9mfXw4aY7C",
        "colab_type": "text"
      },
      "source": [
        "모델 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4f6Vcubegi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98d153c2-e431-4a67-e915-773e4fe621bd"
      },
      "source": [
        "import codecs\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from chardet import detect\n",
        "import keras\n",
        "from keras_radam import RAdam\n",
        "from keras import backend as K\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "# from google.colab import drive\n",
        "\n",
        "SEQ_LEN = 80\n",
        "LR = 2e-4\n",
        "\n",
        "\n",
        "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
        "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
        "\n",
        "try:\n",
        "  model = load_trained_model_from_checkpoint(\n",
        "        config_path,\n",
        "        checkpoint_path,\n",
        "        training=True,\n",
        "        trainable=True,\n",
        "        seq_len=SEQ_LEN,\n",
        "    )\n",
        "except:\n",
        "  !wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "  !unzip -o uncased_L-12_H-768_A-12.zip\n",
        "\n",
        "  model = load_trained_model_from_checkpoint(\n",
        "        config_path,\n",
        "        checkpoint_path,\n",
        "        training=True,\n",
        "        trainable=True,\n",
        "        seq_len=SEQ_LEN,\n",
        "    )\n",
        "\n",
        "\n",
        "inputs = model.inputs[:2]\n",
        "dense = model.get_layer('NSP-Dense').output\n",
        "outputs = keras.layers.Dense(units=7, activation='softmax')(dense)\n",
        "\n",
        "model = keras.models.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "compiled = model.compile(\n",
        "  RAdam(learning_rate =LR),\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, 80)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, 80)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 80, 768), (3 23440896    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 80, 768)      1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 80, 768)      0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 80, 768)      61440       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 80, 768)      0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 80, 768)      1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 80, 768)      2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 80, 768)      0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 80, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 80, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 80, 768)      0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 80, 768)      0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 80, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 80, 768)      2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 80, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 80, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 80, 768)      0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 80, 768)      0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 80, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 80, 768)      2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 80, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 80, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 80, 768)      0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 80, 768)      0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 80, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 80, 768)      2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 80, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 80, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 80, 768)      0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 80, 768)      0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 80, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 80, 768)      2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 80, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 80, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 80, 768)      0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 80, 768)      0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 80, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 80, 768)      2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 80, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 80, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 80, 768)      0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 80, 768)      0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 80, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 80, 768)      2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 80, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 80, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 80, 768)      0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 80, 768)      0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 80, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 80, 768)      2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 80, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 80, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 80, 768)      0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 80, 768)      0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 80, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 80, 768)      2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 80, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 80, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 80, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 80, 768)      0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 80, 768)      0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 80, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 80, 768)      2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 80, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 80, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 80, 768)      1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 80, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 80, 768)      0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 80, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 80, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 80, 768)      2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 80, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 80, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 80, 768)      1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 80, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 80, 768)      0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 80, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 80, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 80, 768)      2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 80, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 80, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 80, 768)      1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 80, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 80, 768)      0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 80, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 80, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            5383        NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,155,847\n",
            "Trainable params: 109,155,847\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmXGxA_hbF4_",
        "colab_type": "text"
      },
      "source": [
        "러닝 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwEWXpRJlwr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these sentences aren't that long so we may as well use the whole string\n",
        "sequence_length = SEQ_LEN # 이 길이를 넘은 문장은 걍 짤림\n",
        "max_features = 20000 # this is the number of words we care about\n",
        "\n",
        "import codecs\n",
        "from keras_bert import Tokenizer\n",
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "\n",
        "tokenizer = Tokenizer(token_dict)\n",
        "# X 데이터를 만들어줌\n",
        "def generate_X(df):\n",
        "  global tokenizer\n",
        "  indices = []\n",
        "  for utterance in df['utterance'].values:\n",
        "    ids, segments = tokenizer.encode(utterance, max_len=SEQ_LEN)\n",
        "    indices.append(ids)\n",
        "  \n",
        "  indices = np.array(indices)\n",
        "  return [indices, np.zeros_like(indices)]\n",
        "\n",
        "\n",
        "# Y 데이터를 만들어줌\n",
        "def generate_Y(df):\n",
        "  annotations = df['annotation'].apply(lambda x: pd.Series([float(c)/5 for c in x], dtype='float32'))\n",
        "  # 어노테이션 값을 벡터화함.\n",
        "  # 2120000 ->벡터화-> [2, 1, 2, 0, 0, 0, 0] ->나누기 5-> [0.4, 0.2, 0.4, 0, 0, 0, 0]\n",
        "  return annotations.values\n",
        "\n",
        "# X 데이터를 만들어줌\n",
        "def generate_train_xy(df):\n",
        "  x = generate_X(df)\n",
        "  y = generate_Y(df)\n",
        "  \n",
        "  indices = x[0]\n",
        "  mod = indices.shape[0] % 32\n",
        "  if mod > 0:\n",
        "      indices = indices[:-mod]\n",
        "\n",
        "  return [indices, np.zeros_like(indices)], y[:-mod]\n",
        "\n",
        "\n",
        "X_train, y_train = generate_train_xy(train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc0O0IgqbTEH",
        "colab_type": "text"
      },
      "source": [
        "실행\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CWzWBcQ562_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "outputId": "f0990f79-e063-4914-9466-d86d7a8b896c"
      },
      "source": [
        "learn()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean length of sentence: 60.91280752532562\n",
            "max length of sentence: 286\n",
            "std dev length of sentence: 47.66618446797404\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'neutral': 1130, 'non-neutral': 1502, 'sadness': 128, 'joy': 4}\n",
            "accuray:  0.32850940665701883\n",
            "f1_score:  0.09980688268021566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "10560/10560 [==============================] - 351s 33ms/step - loss: 1.8143 - accuracy: 0.4683\n",
            "Epoch 2/3\n",
            "10560/10560 [==============================] - 340s 32ms/step - loss: 1.8141 - accuracy: 0.4667\n",
            "Epoch 3/3\n",
            "10560/10560 [==============================] - 340s 32ms/step - loss: 1.8165 - accuracy: 0.4669\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'neutral': 1191, 'non-neutral': 1445, 'sadness': 124, 'joy': 4}\n",
            "accuray:  0.3357452966714906\n",
            "f1_score:  0.10114689021699111\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "seq of accuracies: [0.3357452966714906]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
